{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-526d7954e645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_moons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch\n",
    "\n",
    "Today, you'll learn how to code and train a neural network from scratch using just `numpy` and your brain. \n",
    "\n",
    "> This notebook closely follows the exercises and content on the course website (chapter 10.1 and 10.2)\n",
    "\n",
    "\n",
    "Let's start with some toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=50, noise=0.1, random_state=42)\n",
    "y = y.reshape(-1, 1) # make y a column vector\n",
    "plt.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap questions\n",
    "\n",
    "- How many observations does your data have? How many input features?\n",
    "- For this classification task, why does a simple Logistic Regression (LogReg) model performance poorly?\n",
    "- How many model parameters (weights) does a LogReg model have for this task?\n",
    "- Which feature engineering could you apply to solve this classification problem with a LogReg model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of a Supervised Learning model\n",
    "\n",
    "1. A prediction function that maps the input `X` to the output `y`: $F(X;w) = \\hat{y}$\n",
    "2. A loss function that evaluates the goodness of fit: $L(y, \\hat{y})$\n",
    "3. Training data that is used to find the weights `w` that minimize the loss function. This is done via the Gradient Descent Algorithm:\n",
    "\n",
    "    $$\n",
    "    w_{new} = w_{old} - LR \\cdot \\nabla_L(w)\n",
    "    $$\n",
    "\n",
    "4. Separate validation data that is used to assess the model's performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with a Log Reg model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of ones to the input data. why are we doing this?\n",
    "\n",
    "def add_bias(X):\n",
    "    return np.hstack([X, np.ones((X.shape[0], 1))])\n",
    "\n",
    "X = add_bias(X)\n",
    "\n",
    "\n",
    "assert X.shape[1] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "F(X) = w_0X_0 + w_1X_1 + w_21 = Xw\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize some random weights from the normal distribution\n",
    "w = np.random.randn(3,1)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the linear predictor (the linear combination between the input and the weights)\n",
    "# X[:,0]*w[0] + X[:,1]*w[1] + w[2]\n",
    "X.dot(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sigmoid non linear transformation\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "a = np.array([-10.0, -1.0, 0.0, 1.0, 10.0])\n",
    "expected = np.array([0.0, 0.27, 0.5, 0.73, 1.0])\n",
    "assert np.all(sigmoid(a).round(2) == expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the log loss (aka binary crossentropy)\n",
    "\n",
    "def log_loss(y, y_pred):\n",
    "    return - (y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "a = np.array([0.0, 0.0, 1.0, 1.0])\n",
    "b = np.array([0.01, 0.99, 0.01, 0.99])\n",
    "expected = np.array([0.01, 4.61, 4.61, 0.01])\n",
    "assert np.all(log_loss(a, b).round(2) == expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Making predictions with a neural net (the feed forward function)\n",
    "\n",
    "We build a Neural Net with \n",
    "\n",
    "- one hidden layer that contains 3 \"neurons\"/ units\n",
    "- one output layer with 1 unit\n",
    "- a `sigmoid` activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how can we calculate the output of several LogReg models at the same time? \n",
    "# this is the first layer of a neural net!\n",
    "\n",
    "weights = []\n",
    "\n",
    "weights.append(np.random.randn(3,3)) # 3 units\n",
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer\n",
    "X_hidden = sigmoid(X.dot(weights[0]))\n",
    "X_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed the output of the first hidden layer into a second layer! this is an ordinary logistic regression.\n",
    "\n",
    "X_hidden_with_bias = add_bias(X_hidden)\n",
    "\n",
    "weights.append(np.random.randn(4, 1))  # 3 for each hidden feature + 1 bias weight\n",
    "\n",
    "sigmoid(X_hidden_with_bias.dot(weights[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine everything in one function\n",
    "\n",
    "def feed_forward(X, weights):\n",
    "\n",
    "    \"\"\"\n",
    "    1. Calculate the dot product of X\n",
    "       and the weights of the first layer.\n",
    "\n",
    "    2. Apply the sigmoid function on the result.\n",
    "\n",
    "    3. Append an extra column of ones to the result (i.e. the bias).\n",
    "\n",
    "    4. Calculate the dot product of the previous step\n",
    "       with the weights of the second (i.e. outer) layer.\n",
    "\n",
    "    5. Apply the sigmoid function on the result.\n",
    "\n",
    "    6. Return all intermediate results (i.e. anything that is outputted\n",
    "       by an activation function).\n",
    "    \"\"\"\n",
    "    \n",
    "    output1 = sigmoid(np.dot(X, weights[0]))   \n",
    "    output2 = sigmoid(np.dot(add_bias(output1), weights[1]))\n",
    "    return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize some random weights\n",
    "\n",
    "weights = [\n",
    "    np.random.randn(3, 3),\n",
    "    np.random.rand(4, 1)\n",
    "]\n",
    "\n",
    "# testing \n",
    "\n",
    "out1, out2 = feed_forward(X, weights)\n",
    "\n",
    "assert out1.shape == (50, 3)\n",
    "assert out2.shape == (50, 1)\n",
    "\n",
    "Xref = np.array([[1.0, 2.0, 1.0]])\n",
    "whidden = np.array([[1.0, 2.0, 0.0],\n",
    "                 [-1.0, -2.0, 0.0]\n",
    "                    ]).T\n",
    "wout = np.array([[1.0, -1.0, 0.5]]).T\n",
    "\n",
    "out1, out2 = feed_forward(Xref, [whidden, wout])\n",
    "assert np.all(out1.round(2) == np.array([[0.99, 0.01]]))\n",
    "assert np.all(out2.round(2) == np.array([[0.82]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Training a Neural Net via Gradient Descent and Backpropagation\n",
    "\n",
    "Backpropagation is the fancy name for calculating the gradient (partial derivative) of the loss function with respect to all its weights in every layer. To make this easier we first calculate the derivatives for the indiviual parts of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_der(X):\n",
    "    \"\"\"derivative of sigmoid with respect to X\"\"\"\n",
    "    return sigmoid(X) * (1-sigmoid(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y, y_pred):\n",
    "    return - (y*np.log(y_pred) + (1-y)*np.log(1-y_pred))\n",
    "\n",
    "def log_loss_der(y, y_pred):\n",
    "    \"\"\"derivative of log loss with respect to y_pred\"\"\"\n",
    "    return - ((y*1/y_pred) + (1-y)*(1/(1-y_pred))*(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(weights,\n",
    "             output1,\n",
    "             output2,\n",
    "             ytrue,\n",
    "             X_input,\n",
    "             LR):    \n",
    "\n",
    "    wH = weights[0]\n",
    "    wO = weights[1]\n",
    "\n",
    "    '''EQUATION A:'''\n",
    "    # error = (output2 - ytrue) * log_loss(ytrue , output2)\n",
    "    error = log_loss_der(ytrue, output2)\n",
    "\n",
    "    '''EQUATION B:'''\n",
    "    #derivative of the sigmoid function with respect to the\n",
    "    #hidden output * weights\n",
    "    hidden_out_with_bias = add_bias(output1)\n",
    "    y_grad = sigmoid_der(hidden_out_with_bias.dot(wO)) * error\n",
    "    \n",
    "\n",
    "    '''EQUATION C:'''\n",
    "    # hidden_out_with_bias = add_bias(output1)\n",
    "    #don't forget the bias!\n",
    "    delta_wO = - np.dot(y_grad.T, hidden_out_with_bias ) * LR\n",
    "\n",
    "    #and finally, old weights + delta weights -> new weights!\n",
    "    wO_new = wO + delta_wO.T\n",
    "\n",
    "    '''EQUATION D:'''\n",
    "    H_grad = sigmoid_der(X_input.dot(wH))  * y_grad.dot(wO[:-1].T)\n",
    "    #exclude the bias (3rd column) of the outer weights,\n",
    "    #since it is not backpropagated!\n",
    "\n",
    "    '''EQUATION E:'''\n",
    "    delta_wH = -np.dot(H_grad.T, X_input) * LR\n",
    "    wH_new = wH + delta_wH.T\n",
    "    #old weights + delta weights -> new weights!\n",
    "\n",
    "    return wH_new, wO_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "X = add_bias(X)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "weights = [\n",
    "   np.random.normal(size=(3, 4)), # 4 neurons in the hidden layer\n",
    "   np.random.normal(size=(5, 1))\n",
    "]\n",
    "\n",
    "LOSS_VEC = []    \n",
    "for i in range(500):\n",
    "    out1, out2 = feed_forward(X, weights)\n",
    "    LOSS_VEC.append(log_loss(y, out2).sum())\n",
    "    new_weights = backprop(weights, out1, out2, y, X, 0.01)\n",
    "    weights = new_weights\n",
    "plt.plot(LOSS_VEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-3, 3, 200)\n",
    "X_vis = np.array([(x1, x2) for x1 in x for x2 in x])\n",
    "X_vis = add_bias(X_vis)\n",
    "_, y_pred = feed_forward(X_vis, weights)\n",
    "Z = y_pred.reshape((len(x), len(x)), order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1)\n",
    "cp = ax.contourf(x, x, Z, alpha=0.8, cmap='coolwarm')\n",
    "ax.contour(x, x, Z, levels=[0.5])\n",
    "fig.colorbar(cp) # Add a colorbar to a plot\n",
    "ax.scatter(X[:,0], X[:,1], c=y, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Neural Nets with Keras (tensorflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup the model and its layers\n",
    "# Step 2: Compile the model (optimization algorithm)\n",
    "# Step 3: Fit the model on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "model = keras.models.Sequential()\n",
    "# input_dim only defined at the first layer\n",
    "model.add(keras.layers.Dense(units=4, activation=keras.activations.sigmoid, input_dim=2))\n",
    "model.add(keras.layers.Dense(units=1, activation=keras.activations.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.3), \n",
    "              loss=keras.losses.binary_crossentropy,  # just another name for the log-loss\n",
    "              metrics=[keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "\n",
    "hist = model.fit(X, y, \n",
    "          epochs=200,       # number of iterations over all datapoints\n",
    "          batch_size=200  # number of observation to use in each weight update\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist.history['loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
